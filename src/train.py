"""
Module for training various ensemble trees on a tabular dataset created by STAD-FEBTE. 
"""
# %%
import warnings
import os
import argparse
import yaml
import pickle
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score
from sklearn.metrics import recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix
from utils import save_data, warn


# %%
def load_data(tab_path_, train_on_FE, train_on_FS):
    """
    Loads the tabular dataset from tab_path_

    Args:
        tab_path_ (str): path of the tabular dataset generated by STAD-FEBTE
        train_on_FE (Boolean): whether to train the model on the output of FE module
        train_on_FS (Boolean): whether to train the model on the output of FS module

    Returns:
        dataset_tr ([np.array]): list of training design matrices
        dataset_ts ([np.array]): list of test design matrices
        label_tr ([np.array]): list of training output vectors
        label_ts ([np.array]): list of test output vectors
        data_names ([str]): list of dataset names:
            "tab": tabular dataset created by FE module
            "tab_fs": filtered tabular dataset created by FE+FS modules
            "tab_fs_b": balanced filtered tabular dataset created by FE+FS+AG modules
    """
    with open(tab_path_, "rb") as f:
        dataset = pickle.load(f)

    dataset_tr = [dataset["X_tr_tf_b"].to_numpy()]
    label_tr = [dataset["y_tr_b"].to_numpy()]
    dataset_ts = [dataset["X_ts_tf"].to_numpy()]
    label_ts = [dataset["y_ts"].to_numpy()]
    data_names = ["tab_fs_b"]

    if train_on_FE and not train_on_FS:
        dataset_tr.append(dataset["X_tr_t"].to_numpy())
        label_tr.append(dataset["y_tr"].to_numpy())
        dataset_ts.append(dataset["X_ts_t"].to_numpy())
        label_ts.append(dataset["y_ts"].to_numpy())
        data_names = ["tab_fs_b", "tab"]

    elif not train_on_FE and train_on_FS:
        dataset_tr.append(dataset["X_tr_tf"].to_numpy())
        label_tr.append(dataset["y_tr"].to_numpy())
        dataset_ts.append(dataset["X_ts_tf"].to_numpy())
        label_ts.append(dataset["y_ts"].to_numpy())
        data_names = ["tab_fs_b", "tab_fs"]

    elif train_on_FE and train_on_FS:
        dataset_tr.extend([dataset["X_tr_tf"].to_numpy(), dataset["X_tr_t"].to_numpy()])
        label_tr.extend([dataset["y_tr"].to_numpy(), dataset["y_tr"].to_numpy()])
        dataset_ts.extend([dataset["X_ts_tf"].to_numpy(), dataset["X_ts_t"].to_numpy()])
        label_ts.extend([dataset["y_ts"].to_numpy(), dataset["y_ts"].to_numpy()])
        data_names = ["tab_fs_b", "tab_fs", "tab"]

    dataset_tr = list(reversed(dataset_tr))
    dataset_ts = list(reversed(dataset_ts))
    label_tr = list(reversed(label_tr))
    label_ts = list(reversed(label_ts))
    data_names = list(reversed(data_names))
    
    return dataset_tr, dataset_ts, label_tr, label_ts, data_names


# %%
def report_metrics(model, X, y, model_name):
    """
    Computes various performance metrics for a trained classifier.

    Args:
        model (sklearn classifier): trained sklearn classifier object
        X (np.ndarray): design matrix of test dataset
        y (np.ndarray): output vector of test dataset
        model_name (str): name of the model, appears in saved report
        
    Returns:
        report (dict): created report holding following keys: 
                        - "model_name": name of the model
                        - "accuracy": model accuracy
                        - "balanced_accuracy": model balanced accuracy
                        - "recall": model recall
                        - "f1": model f1 score
                        - "roc_auc": model ROC-AUC score
                        - "cr": model classification report
                        - "cm": model confusion matrix 
    """

    report = dict()
    report["model_name"] = model_name

    acc = accuracy_score(y, model.predict(X)) 
    report["accuracy"] = acc

    b_acc = balanced_accuracy_score(y, model.predict(X)) 
    report["balanced_accuracy"] = b_acc

    rec = recall_score(y, model.predict(X), average="weighted") 
    report["recall"] = rec

    f1 = f1_score(y, model.predict(X), average="weighted") 
    report["f1"] = f1

    roc = roc_auc_score(y, model.predict_proba(X), average="weighted",
                        multi_class="ovr") 
    report["roc"] = roc

    cr = classification_report(y, model.predict(X)) 
    report["cr"] = cr

    cm = confusion_matrix(y, model.predict(X), normalize="true")
    report["cm"] = cm

    return report


# %%
def train_defaultHP(model_names, dataset_tr, dataset_ts, label_tr, label_ts, data_names, n_estimators, n_jobs, target_path):
    """
    Trains various tree-based ensembles on a given dataset with default HPT.

    Args:
        model_names ([str]): model names. Should be in ["bagging", "rf", "extra_trees", "ada_boost", "grad_boost"]
        dataset_tr ([np.array]): list of training design matrices
        dataset_ts ([np.array]): list of test design matrices
        label_tr ([np.array]): list of training output vectors
        label_ts ([np.array]): list of test output vectors
        data_names ([str]): list of dataset names:
            "tab": tabular dataset created by FE module
            "tab_fs": filtered tabular dataset created by FE+FS modules
            "tab_fs_b": balanced filtered tabular dataset created by FE+FS+AG modules
        n_estimators (int): number of estimators of the ensemble model
        n_jobs (int): number of CPUs
        target_path (str): path to save trained models

    Returns:
        model: trained sklearn classifier
    """
    
    # model_name to estimator map
    models = dict(
        bagging=BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=None),
                                bootstrap_features=True,
                                n_estimators=n_estimators,
                                n_jobs=n_jobs),
        rf=RandomForestClassifier(n_estimators=n_estimators,
                                n_jobs=n_jobs),
        extra_trees=ExtraTreesClassifier(n_estimators=n_estimators,
                                        n_jobs=n_jobs),
        ada_boost=AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),
                                    n_estimators=n_estimators),
        grad_boost=HistGradientBoostingClassifier()
    )

    reports = pd.DataFrame()

    for model_name in model_names:
        print("")
        for data_name, X_tr, X_ts, y_tr, y_ts in zip(data_names, dataset_tr, dataset_ts, label_tr, label_ts):
            # Train
            print(f"training {model_name} on data_{data_name} ...")
            model = models[model_name.lower()]
            model.fit(X_tr, y_tr)
            save_data(model, model_name + "_" + data_name, target_path, extension=".pickle")
            
            # Validate
            print(f"validating {model_name} on {data_name} ...")
            report_ = report_metrics(model, X_ts, y_ts, model_name)
            save_data(report_, model_name + "_" + data_name, target_path, extension=".json")

            # Create report
            report = {key:report_[key] for key in ["model_name", "accuracy", "balanced_accuracy", "recall", "f1", "roc"]}
            report = pd.DataFrame([report])
            reports = pd.concat([reports, report], axis=0)
    
    return reports


# %%
def main(config_data):
    # Read config data
    tab_path_ = config_data["train"]["tab_path_"]
    model_path = config_data["train"]["model_path"]
    model_names = config_data["train"]["model_names"]
    train_on_FE = config_data["train"]["train_on_FE"]
    train_on_FS = config_data["train"]["train_on_FS"]
    n_estimators = config_data["train"]["n_estimators"]
    n_jobs = config_data["train"]["n_jobs"]

    # Create target_path
    now = datetime.now().strftime("%Y_%m_%d - %H_%M")
    target_path = os.path.join(model_path, now)

    # Load data
    print(f"loading the tabular dataset in {tab_path_}...")
    dataset_tr, dataset_ts, label_tr, label_ts, data_names = load_data(tab_path_, train_on_FE, train_on_FS)
    
    # Train
    reports = train_defaultHP(model_names, dataset_tr, dataset_ts, label_tr, label_ts, data_names, n_estimators, n_jobs, target_path)

    # Save output
    print(f"saving results in {target_path}")
    save_data(config_data, "1. config_data", target_path, ".json")
    save_data(reports, "2. reports", target_path, extension=".csv")


# %%
if __name__ == "__main__":
    warnings.warn = warn  # to silence scikit-learn warnings

    parser = argparse.ArgumentParser(description="Recieves the path of the config_data.yaml")
    parser.add_argument("--config_path", type=str, default="../config/config_data.yaml")
    args = parser.parse_args()
    
    with open(args.config_path, "r") as f:
        config_data = yaml.load(f, Loader=yaml.Loader)
    main(config_data)
